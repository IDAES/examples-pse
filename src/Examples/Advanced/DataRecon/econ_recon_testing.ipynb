{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reconciliation for a Single Unit - Economizer\n",
    "\n",
    "This notebook demonstrates data reconciliation with a single unit model, an economizer.  Data for this example was generated by adding noise to supercritical power plant simulations.\n",
    "\n",
    "### Why reconcile data?\n",
    "\n",
    "Data reconciliation uses mass and energy balances along with redundant measurements to improve data quality by:\n",
    "\n",
    "1. reducing measurement error,\n",
    "2. ensuring measurements satisfy mass and energy balances, and\n",
    "3. filling in unmeasured quantities.\n",
    "\n",
    "Data reconciliation is used to refine process data before parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Read Plant Data\n",
    "\n",
    "The first step is to read in process data.  In this case, data was simulated by adding measurement error to supercritical steam cycle simulation results.  IDAES includes functions to read process data, convert units to match a model, and map data to a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDAES module with functions to read, analyze and visualize plant data\n",
    "import idaes.core.dmf.model_data as da\n",
    "# Suppress some warnings\n",
    "from idaes.logger import getLogger\n",
    "import logging\n",
    "getLogger('idaes.core').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data is contained in two CSV files, a data file and a metadata file.  The first column in the data file is row indexes and the first row is process measurement tags.  The index column has an entry for each data row, and is often a time-stamp. The data file format is illustrated by the table below.\n",
    "\n",
    "|        | tag1      | tag2      | tag3       | ... |\n",
    "|--------|-----------|-----------|------------|-----|\n",
    "| index1 | data(1,1) | data(1,2) | data(1, 3) | ... |\n",
    "| index2 | data(2,1) | data(2,2) | data(2, 3) | ... |\n",
    "| ...    | ...       | ...       | ...        | ... |\n",
    "\n",
    "The metadata file contains information about the tags including units of measurement, description, and model mapping information.  The meta data format is show below, any of the columns my be empty.\n",
    "\n",
    "|       |                   |               |                   |                                                     |\n",
    "|-------|--- ---------------|---------------|-------------------|-----------------------------------------------------|\n",
    "| tag1  | model reference 1 | description 1 | unit of measure 1 | Additional comments, additional columns are ignored |\n",
    "| tag2  | model reference 2 | description 2 | unit of measure 2 | ...                                                 |\n",
    "| tag3  | model reference 3 | description 3 | unit of measure 3 | ...                                                 |\n",
    "| ...   | ...               | ...           | ...               | ...                                                 |\n",
    "\n",
    "Once the process data is read in, the data is assigned to bins based on the value in a given column, in this case gross power. Dividing the data into bins allows rough estimation of measurement uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and column metadata\n",
    "df, df_meta = da.read_data(\"plant_data.csv\", \"plant_data_meta.csv\")\n",
    "\n",
    "# Add bin information where the data is sorted into 5 MW bins based on the \"GROSS_POWER\" column\n",
    "# A bin number column is added along with a column for nominal gross power in the bin. \n",
    "bin_count = da.bin_data(df, bin_by=\"POWER_GROSS\", bin_no=\"bin_no\", bin_nom=\"bin_power\", bin_size=5e6)\n",
    "\n",
    "# Calculate the standard deviation by bin for each column.  The resulting standard devations can be \n",
    "# accessed like so: bin_stdev[bin number][column name]\n",
    "bin_stdev = da.bin_stdev(df, bin_no=\"bin_no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to visualize the measurement data and estimated uncertainty.  The following creates box and whisker plots for each tag based on the data bins.  A large number of plots may be created, so to manage them more easily, they are saved as PDFs and merged into a single multi-page PDF document.  The deafault file name for the resulting PDF is \"data_plot_book.pdf.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pdf book of plots that shows box and whisker plots for each column by bin\n",
    "import os\n",
    "if not os.path.isfile(\"data_plot_book.pdf\"):\n",
    "    da.data_plot_book(df, bin_nom=\"bin_power\", xlabel=\"gross power (W)\", metadata=df_meta, file=\"data_plot_book.pdf\")\n",
    "# There should now be a data_plot_book.pdf file in this directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Create Unit model\n",
    "\n",
    "Now that we have the plant data, we need to create a unit model that we can use for data reconciliation.  Although we need a model that has just mass and energy balances and feasibility constraints for the data reconciliation problem, we start with the full economizer model here.  Using the same model for data reconciliation, parameter estimation, validation, and simulation reduces the work required to move between steps in the workflow.\n",
    "\n",
    "Once the full model is created, constraints that are not needed for data reconciliation can be deactivated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models\n",
    "from idaes.core import FlowsheetBlock\n",
    "from idaes.models_extra.power_generation.properties.flue_gas_ideal import FlueGasParameterBlock\n",
    "from idaes.models.properties import iapws95\n",
    "from idaes.models_extra.power_generation.unit_models.boiler_heat_exchanger import (\n",
    "    BoilerHeatExchanger, \n",
    "    TubeArrangement, \n",
    "    DeltaTMethod\n",
    ")\n",
    "import pyomo.environ as pyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flowsheet with economizer\n",
    "m = pyo.ConcreteModel()\n",
    "m.fs = FlowsheetBlock(dynamic=False)\n",
    "m.fs.prop_water = iapws95.Iapws95ParameterBlock()\n",
    "m.fs.prop_fluegas = FlueGasParameterBlock()\n",
    "\n",
    "m.fs.econ = BoilerHeatExchanger(\n",
    "        side_1_property_package=m.fs.prop_water,\n",
    "        side_2_property_package=m.fs.prop_fluegas,\n",
    "        has_pressure_change=True,\n",
    "        has_holdup=False,\n",
    "        delta_T_method=DeltaTMethod.counterCurrent,\n",
    "        tube_arrangement=TubeArrangement.inLine,\n",
    "        side_1_water_phase=\"Liq\",\n",
    "        has_radiation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and initialize the model\n",
    "\n",
    "# The steam properties use enthalpy as a state variable, so use the known \n",
    "# temperature and pressure to calculate the feedwater inlet enthalpy\n",
    "h = pyo.value(iapws95.htpx(563.706*pyo.units.K, 2.5449e7*pyo.units.Pa))\n",
    "\n",
    "m.fs.econ.side_1_inlet.flow_mol[0].fix(24678.26) # mol/s\n",
    "m.fs.econ.side_1_inlet.enth_mol[0].fix(h) #J/mol         \n",
    "m.fs.econ.side_1_inlet.pressure[0].fix(2.5449e7) # Pa\n",
    "\n",
    "# Set the flue gas flow and composition\n",
    "fg_rate = 28.3876e3  # mol/s equivalent of ~1930.08 klb/hr\n",
    "fg_comp = { # mol fraction of flue gas components\n",
    "    \"H2O\":8.69/100,\n",
    "    \"CO2\":14.49/100,\n",
    "    \"O2\":2.47/100,\n",
    "    \"NO\":0.0006,\n",
    "    \"SO2\":0.002,\n",
    "}\n",
    "# The rest is N2\n",
    "fg_comp[\"N2\"] = 1 - sum(fg_comp[i] for i in fg_comp)\n",
    "\n",
    "# Set economizer inlets\n",
    "for c in fg_comp:\n",
    "    m.fs.econ.side_2.properties_in[0].flow_mol_comp[c].fix(fg_rate*fg_comp[c])    \n",
    "m.fs.econ.side_2_inlet.temperature[0].fix(682.335)  # K\n",
    "m.fs.econ.side_2_inlet.pressure[0].fix(100145)  # Pa\n",
    "\n",
    "# Set economizer design variables and parameters\n",
    "ITM = 0.0254  # inch to meter conversion\n",
    "# Based on NETL Baseline Report Rev4\n",
    "m.fs.econ.tube_thickness.fix(0.188*ITM)  # tube thickness\n",
    "m.fs.econ.tube_di.fix((2.0 - 2.0 * 0.188)*ITM) # calc inner diameter\n",
    "m.fs.econ.pitch_x.fix(3.5*ITM)\n",
    "m.fs.econ.pitch_y.fix(5.03*ITM)\n",
    "m.fs.econ.tube_length.fix(53.41*12*ITM)  # use tube length (53.41 ft)\n",
    "m.fs.econ.tube_nrow.fix(36*2.5) # use to match baseline performance\n",
    "m.fs.econ.tube_ncol.fix(130) # 130 from thermoflow\n",
    "m.fs.econ.nrow_inlet.fix(2)\n",
    "m.fs.econ.delta_elevation.fix(50)\n",
    "m.fs.econ.tube_r_fouling = 0.000176\n",
    "m.fs.econ.shell_r_fouling = 0.00088\n",
    "m.fs.econ.fcorrection_htc.fix(1.5)\n",
    "m.fs.econ.fcorrection_dp_tube.fix(1.0)\n",
    "m.fs.econ.fcorrection_dp_shell.fix(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize economizer\n",
    "m.fs.econ.initialize(\n",
    "    state_args_1={\n",
    "        \"flow_mol\": m.fs.econ.side_1_inlet.flow_mol[0].value,\n",
    "        \"pressure\": m.fs.econ.side_1_inlet.pressure[0].value,\n",
    "        \"enth_mol\": m.fs.econ.side_1_inlet.enth_mol[0].value,\n",
    "    },\n",
    "    state_args_2={\n",
    "        \"flow_component\":{\n",
    "            \"H2O\": m.fs.econ.side_2_inlet.flow_mol_comp[0, \"H2O\"].value,\n",
    "            \"CO2\": m.fs.econ.side_2_inlet.flow_mol_comp[0, \"CO2\"].value,\n",
    "            \"N2\": m.fs.econ.side_2_inlet.flow_mol_comp[0, \"N2\"].value,\n",
    "            \"O2\": m.fs.econ.side_2_inlet.flow_mol_comp[0, \"O2\"].value,\n",
    "            \"NO\": m.fs.econ.side_2_inlet.flow_mol_comp[0, \"NO\"].value,\n",
    "            \"SO2\": m.fs.econ.side_2_inlet.flow_mol_comp[0, \"SO2\"].value,\n",
    "        },\n",
    "        \"temperature\": m.fs.econ.side_2_inlet.temperature[0].value,\n",
    "        \"pressure\": m.fs.econ.side_2_inlet.pressure[0].value,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Simplify to Mass and Energy Balances\n",
    "\n",
    "For data reconciliation, the model should be reduced to mass and energy balances and potentially limited performance constraints to keep the results feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deactivate constraints for heat transfer\n",
    "m.fs.econ.overall_heat_transfer_coefficient_eqn.deactivate()\n",
    "m.fs.econ.rcond_wall_eqn.deactivate()\n",
    "m.fs.econ.hconv_shell_total_eqn.deactivate()\n",
    "m.fs.econ.hconv_shell_conv_eqn.deactivate()\n",
    "m.fs.econ.N_Nu_shell_eqn.deactivate()\n",
    "m.fs.econ.N_Pr_shell_eqn.deactivate()\n",
    "m.fs.econ.deltaP_shell_eqn.deactivate()\n",
    "m.fs.econ.friction_factor_shell_eqn.deactivate()\n",
    "m.fs.econ.N_Re_shell_eqn.deactivate()\n",
    "m.fs.econ.v_shell_eqn.deactivate()\n",
    "m.fs.econ.hconv_tube_eqn.deactivate()\n",
    "m.fs.econ.N_Nu_tube_eqn.deactivate()\n",
    "m.fs.econ.N_Pr_tube_eqn.deactivate()\n",
    "m.fs.econ.deltaP_tube_eqn.deactivate()\n",
    "m.fs.econ.deltaP_tube_uturn_eqn.deactivate()\n",
    "m.fs.econ.deltaP_tube_friction_eqn.deactivate()\n",
    "m.fs.econ.friction_factor_tube_eqn.deactivate()\n",
    "m.fs.econ.N_Re_tube_eqn.deactivate()\n",
    "m.fs.econ.v_tube_eqn.deactivate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Map Data to the Model\n",
    "\n",
    "Although the model mapping can be included in the tag metadata file, here we just add the mapping information to the tag metadata after reading the data. Sometime a data set may be used with more than one model or you may want to examine data before creating a model, in which case it is convenient to defer mapping the data to the model as we have done here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta[\"ECON_OUT_F\"][\"reference_string\"] = \"m.fs.econ.side_1.properties_out[:].flow_mol\"\n",
    "df_meta[\"ECON_OUT_T\"][\"reference_string\"] = \"m.fs.econ.side_1.properties_out[:].temperature\"\n",
    "df_meta[\"ECON_OUT_P\"][\"reference_string\"] = \"m.fs.econ.side_1.properties_out[:].pressure\"\n",
    "df_meta[\"BFW_F\"][\"reference_string\"] = \"m.fs.econ.side_1.properties_in[:].flow_mol\"\n",
    "df_meta[\"BFW_T\"][\"reference_string\"] = \"m.fs.econ.side_1.properties_in[:].temperature\"\n",
    "df_meta[\"BFW_P\"][\"reference_string\"] = \"m.fs.econ.side_1.properties_in[:].pressure\"\n",
    "df_meta[\"FG_2_ECON_Fm\"][\"reference_string\"] = \"m.fs.econ.side_2.properties_in[:].flow_mass\"\n",
    "df_meta[\"FG_2_ECON_T\"][\"reference_string\"] = \"m.fs.econ.side_2.properties_in[:].temperature\"\n",
    "df_meta[\"FG_2_ECON_P\"][\"reference_string\"] = \"m.fs.econ.side_2.properties_in[:].pressure\"\n",
    "df_meta[\"FG_2_AIRPH_Fm\"][\"reference_string\"] = \"m.fs.econ.side_2.properties_out[:].flow_mass\"\n",
    "df_meta[\"FG_2_AIRPH_T\"][\"reference_string\"] = \"m.fs.econ.side_2.properties_out[:].temperature\"\n",
    "df_meta[\"FG_2_AIRPH_P\"][\"reference_string\"] = \"m.fs.econ.side_2.properties_out[:].pressure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the model references to the tag metadata based on the strings above.\n",
    "da.upadate_metadata_model_references(m, df_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of data tags that we want to use for the data reconciliation problem.  \n",
    "# The key is the tag and the value is a reference to a quantity in the model.\n",
    "data_tags = {k:v[\"reference\"][0] for k, v in df_meta.items() if v[\"reference\"] is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for result output, the data reconciliation usually can give full stream information for a flowsheet\n",
    "# including quantities that are unmeasured.  To more easily use the results, it is good practice to map most of\n",
    "# the data reconciliation results to flowsheet stream names.  \n",
    "import idaes.core.util.tables as ta\n",
    "\n",
    "# This function creates a dictionary of streams based of streams based on model arcs.  The function\n",
    "# also takes an addtional set of stream-like objects for add to the stream dictionary.  In this case,\n",
    "# this is a single unit and the flowsheet doesn't contain any arcs, so we add the economized inlet and\n",
    "# outlet ports to the stream dictionary.\n",
    "stream_dict = ta.arcs_to_stream_dict(\n",
    "    m, \n",
    "    additional={\n",
    "        \"BFW\": m.fs.econ.side_1_inlet,\n",
    "        \"ECON_OUT\": m.fs.econ.side_1_outlet,\n",
    "        \"FG_2_ECON\": m.fs.econ.side_2_inlet,\n",
    "        \"FG_2_AIRPH\": m.fs.econ.side_2_outlet,\n",
    "    },\n",
    "    sort=True,\n",
    ")\n",
    "\n",
    "# The next function converts the stream dictionary into a dictionary of state block representing the\n",
    "# streams at a given time point.  In this case, we have a steady state model, so we only have one \n",
    "# time point (0).\n",
    "state_dict = ta.stream_states_dict(stream_dict, time_point=0)\n",
    "\n",
    "# The 'tag_state_quantities()' function below iterates through the state block dictionary and \n",
    "# creates tags for the listed attributes by combining the state block label with the attribute label \n",
    "# in the labels argument.  For example, pressure in the S001 state block would get the tag 'S001_P'.\n",
    "recon_tags = ta.tag_state_quantities(\n",
    "    blocks=state_dict, \n",
    "    attributes=(\n",
    "        \"flow_mass\", \n",
    "        \"flow_mol\", \n",
    "        \"enth_mol\", \n",
    "        \"temperature\", \n",
    "        \"pressure\", \n",
    "        (\"flow_mol_comp\", \"O2\"),\n",
    "        (\"flow_mol_comp\", \"NO\"),\n",
    "        (\"flow_mol_comp\", \"N2\"),\n",
    "        (\"flow_mol_comp\", \"SO2\"),\n",
    "        (\"flow_mol_comp\", \"CO2\"),\n",
    "        (\"flow_mol_comp\", \"H2O\"),\n",
    "    ), \n",
    "    labels=(\"_Fm\", \"_F\", \"_h\", \"_T\", \"_P\", \"_F[O2]\", \"_F[NO]\", \"_F[N2]\", \"_F[SO2]\", \"_F[CO2]\", \"_F[H2O]\"),\n",
    ")\n",
    "\n",
    "# Any addtional tags can be added.  This is required for tags that cannot be systematically generated \n",
    "# from the model streams.\n",
    "recon_tags[\"ECON_Q\"] = m.fs.econ.heat_duty[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. View model flowsheet\n",
    "\n",
    "Model results or other quantities can be added to a process flow diagram.  The PFD was drawn beforehand and the model results are added to tagged locations on the PFD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from idaes.core.util.tags import svg_tag  # utility to place numbers/text in an SVG\n",
    "\n",
    "with open(\"econ.svg\", \"r\") as f:\n",
    "    s = svg_tag(svg=f, tags={\"subtitle\":\"Initialized Model\"})\n",
    "    s = svg_tag(svg=s, tags=recon_tags, outfile=\"econ_init.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display\n",
    "\n",
    "display(SVG(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "testing",
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "testing",
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert pyo.value(recon_tags[\"ECON_Q\"]) == pytest.approx(9.3819e7, rel=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Write Objective\n",
    "\n",
    "Next we write the objective function and additional constraints for the data reconciliation problem.  The objective is\n",
    "\n",
    "$$\\min \\sum_i \\left(\\frac{x_{\\text{data}, i} - x_{\\text{model}, i}}{\\sigma_i} \\right)^2$$\n",
    "\n",
    "Where $i \\in \\{\\text{Measured Quantities}\\}$ and $\\sigma_i$ is the standard deviation of measurement i.  In this case, for lack of better information, the standard deviation was estimated by binning the data and calculating the standard deviation of each measured variable in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model parameters to contain measured data.  These are mutable so we can set a specific data point later.\n",
    "m.data = pyo.Param(data_tags, mutable=True, doc=\"Process data for a specific point in time.\")\n",
    "m.data_stdev = pyo.Param(data_tags, mutable=True, doc=\"Process data standard deviation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'set_data' function below takes data from the process data DataFrame and updates the\n",
    "# data parameters in the model.\n",
    "def set_data(m, df, data_tags, index=None, indexindex=None):\n",
    "    if index is None:\n",
    "        index = df.index[indexindex]\n",
    "    m.bin_no = df.iloc[index][\"bin_no\"]\n",
    "    for t in data_tags:\n",
    "        m.data[t] = df.iloc[index][t]\n",
    "        m.data_stdev[t] = bin_stdev[m.bin_no][t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we have something reasonable to start, set the data attached to the model to the first \n",
    "# data point.\n",
    "set_data(m, df, data_tags, indexindex=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an expression for error divided by the standard deviation, and use it to write the data reconciliation objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@m.Expression(data_tags)\n",
    "def err(m, i):\n",
    "    return (m.data[i] - data_tags[i])/m.data_stdev[i]\n",
    "\n",
    "m.objective = pyo.Objective(expr=sum(m.err[t]**2 for t in m.err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add constraints that ensure reasonable temperature and keep the flue gas composition correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit temperature approach\n",
    "m.c1 = pyo.Constraint(expr=m.fs.econ.deltaT_1[0] >= 1.0)\n",
    "m.c2 = pyo.Constraint(expr=m.fs.econ.deltaT_2[0] >= 1.0)\n",
    "\n",
    "# Constrain flue gas composition\n",
    "m.flow_fg = pyo.Var(initialize=fg_rate)\n",
    "@m.Constraint(fg_comp)\n",
    "def eq_fg_comp(b, c):\n",
    "    return m.fs.econ.side_2.properties_in[0].flow_mol_comp[c] == fg_comp[c]*m.flow_fg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Solve Optimization\n",
    "\n",
    "Now we need to solve the data reconciliation problem for every data point.  The important results are stored in two DataFrames ```df_result```, which contains results tagged based on model stream names to be used in the parameter estimation step and ```df_result_cmp``` which contains reconciled data based on the original measurement tags and can be used to compare the original measurements to the reconciled results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the inlet and outlet ports are unfixed.  We want to leave these free \n",
    "# to best match the data.\n",
    "m.fs.econ.side_1_inlet.unfix()\n",
    "m.fs.econ.side_2_inlet.unfix()\n",
    "m.fs.econ.side_1_outlet.unfix()\n",
    "m.fs.econ.side_2_outlet.unfix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pyomo solver object\n",
    "solver = pyo.SolverFactory('ipopt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Add bin information to reconciliation results so it can be used in parameter estimation\n",
    "df_result = pd.DataFrame(columns=list(recon_tags.keys())+[\"termination\", \"bin_no\", \"bin_power\"], index=df.index)\n",
    "df_result_cmp = pd.DataFrame(columns=list(data_tags.keys())+[\"termination\"], index=df.index)\n",
    "\n",
    "# Loop through each data point and solve the data reconciliation problem. \n",
    "for i in df.index:\n",
    "    set_data(m, df, data_tags, index=i)\n",
    "    res = solver.solve(m)\n",
    "    tc = str(res.solver.termination_condition)\n",
    "    df_result.iloc[i][\"termination\"] = tc\n",
    "    df_result.iloc[i][\"bin_no\"] = df.iloc[i][\"bin_no\"]\n",
    "    df_result.iloc[i][\"bin_power\"] = df.iloc[i][\"bin_power\"]\n",
    "    df_result_cmp.iloc[i][\"termination\"] = tc\n",
    "    for t in recon_tags:\n",
    "        df_result.iloc[i][t] = pyo.value(recon_tags[t])\n",
    "    for t in data_tags:\n",
    "        df_result_cmp.iloc[i][t] = pyo.value(data_tags[t])\n",
    "    # Show something so you can tell progress is happening\n",
    "    print(f\"{i} -- {tc}, objective: {pyo.value(m.objective)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "testing",
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "for i in df_result.index:\n",
    "    assert df_result.iloc[i][\"termination\"] == \"optimal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the reconciled data to be used for parameter estimation\n",
    "df_result.to_csv(\"econ_recon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a new plot book to compare the original data to the reconciled data.\n",
    "    da.data_rec_plot_book(\n",
    "        df_data=df, \n",
    "        df_rec=df_result_cmp,\n",
    "        file=\"econ_data_rec_plot_book.pdf\",\n",
    "        bin_nom=\"bin_power\", \n",
    "        xlabel=\"gross power (W)\", \n",
    "        metadata=df_meta\n",
    "    )\n",
    "except:\n",
    "    print(\"Plotting failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
